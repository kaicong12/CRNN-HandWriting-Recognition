{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaicong12/CRNN-HandWriting-Recognition/blob/main/KC_Handwritting_CRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VnS3m0qS9624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0d234b-798d-4bf0-9d77-1872bfc38f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3DZgoSwrI2p7"
      },
      "outputs": [],
      "source": [
        "!mkdir -p words xml HTR_Using_CRNN/Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BBMP2UJhHTKX"
      },
      "outputs": [],
      "source": [
        "!tar -xzf  \"/content/gdrive/MyDrive/Colab Notebooks/words.tgz\" -C \"/content/words/\"\n",
        "!tar -xzf  \"/content/gdrive/MyDrive/Colab Notebooks/xml.tgz\" -C \"/content/xml/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0GP6gBHzgTI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76276806-fdca-47d3-9ef0-87743f2d4b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_tqdm in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras_tqdm) (4.64.1)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_tqdm) (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfNdezF8eE-v"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j64a_aB0fhMa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import xml.etree.cElementTree as et\n",
        "\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JlSrOk_6fmkk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aERRif4hgZ3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d053ab14-94bf-47ea-d33a-d2517ce0451f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15763554001549453982\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14415560704\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 12916197956927177814\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pVYsZ02xggCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41bb378-9759-4662-dedc-9f241d5ff765"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# This step is to check GPU is available or not.\n",
        "\n",
        "tf.config.experimental.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pwculm5ogjrj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60d99685-c975-4b45-c513-8ed49042bdf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJbSzgCYlUMt"
      },
      "source": [
        "1. Input shape for our architecture having an input image of height 32 and width 128. Here we used seven convolution layers of which 6 are having kernel size (3,3) and the last one is of size (2.2). And the number of filters is increased from 64 to 512 layer by layer. \n",
        "\n",
        "2. Two max-pooling layers are added with size (2,2) and then two max-pooling layers of size (2,1) are added to extract features with a larger width to predict long texts. \n",
        "\n",
        "3. Also, we used batch normalization layers after fifth and sixth convolution layers which accelerates the training process. \n",
        "\n",
        "4. Then we used a lambda function to squeeze the output from conv layer and make it compatible with LSTM layer. Then used two Bidirectional LSTM layers each of which has 128 units. \n",
        "\n",
        "5. This RNN layer gives the output of size (batch_size, 31, 63). Where 63 is the total number of output classes including blank character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCGe6sqKlfnZ"
      },
      "source": [
        "# Loss Function\n",
        "Here, we are using the CTC loss function. CTC loss is very helpful in text recognition problems. It helps us to prevent annotating each time step and help us to get rid of the problem where a single character can span multiple time step which needs further processing if we do not use CTC.\n",
        "\n",
        "A CTC loss function requires four arguments to compute the loss, predicted outputs, ground truth labels, input sequence length to LSTM and ground truth label length. To get this we need to create a custom loss function and then pass it to the model. To make it compatible with our model, we will create a model which takes these four inputs and outputs the loss. This model will be used for training and for testing we will use the model that we have created earlier “act_model”. Let’s see the code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9pxnOvilmlz"
      },
      "source": [
        "# Train the Model\n",
        "To train the model we will use Adam optimizer. Also, we can use Keras callbacks functionality to save the weights of the best model on the basis of validation loss. In model.compile(), you can see that I have only taken y_pred and neglected y_true. This is because I have already taken labels as input to the model earlier. labels as input to the model earlier.\n",
        "\n",
        "Now train your model on 7850 training images and 876 validation images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNYMRQV3mBfy"
      },
      "source": [
        "# Test the Model\n",
        "Our model is now trained with 7850 images. Now its time to test the model. We can not use our training model because it also requires labels as input and at test time we can not have labels. So to test the model we will use ” act_model ” that we have created earlier which takes only one input: test images.\n",
        "\n",
        "As our model predicts the probability for each class at each time step, we need to use some transcription function to convert it into actual texts. Here we used the CTC decoder to get the output text. Let’s see the code:\n",
        "\n",
        "We use Jaro Distance & Ratio method to test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z7uZ43naMUJ"
      },
      "source": [
        "# Create custom Training Pipeline to train with different data size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vb-h5wvA8XTF"
      },
      "outputs": [],
      "source": [
        "class HandwritingDataset:\n",
        "  def __init__(self, xml_dir, png_dir):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        xml_dir (string): path to the folder which stores the xml file (e.g. /content/xml) \n",
        "        word_dir (string): path to the folder which stores the png images of the dataset (e.g. /content/words)\n",
        "    \"\"\"\n",
        "\n",
        "    self.xml_dir = xml_dir\n",
        "    self.png_dir = png_dir\n",
        "    self.max_label_len = 0\n",
        "    self.char_list = \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "  def extract_words_from_directories(self):\n",
        "    words = []\n",
        "    xml_files = os.listdir(self.xml_dir)\n",
        "\n",
        "    for xml_filename in xml_files:\n",
        "      tree=et.parse(osp.join(self.xml_dir, xml_filename))\n",
        "      root=tree.getroot()\n",
        "\n",
        "      for word in root.iter('word'):\n",
        "        text_id = word.attrib['id']\n",
        "\n",
        "        splits = text_id.split(\"-\")\n",
        "        split_0 = splits[0]\n",
        "        split_1 = \"-\".join(splits[:2])\n",
        "        split_2 = \"-\".join(splits[2:])\n",
        "\n",
        "        filepath = osp.join(self.png_dir, split_0, split_1, f\"{text_id}.png\")\n",
        "\n",
        "        # filepath = f\"/content/words/{split_0}/{split_1}/{text_id}.png\"\n",
        "        if not osp.isfile(filepath):\n",
        "          raise Exception(f\"{filepath} not found.\")\n",
        "\n",
        "        words.append({\n",
        "            'id': text_id,\n",
        "            'image_filepath': filepath,\n",
        "            'text': word.attrib['text']\n",
        "        })\n",
        "    \n",
        "    return words\n",
        "\n",
        "  def encode_to_labels(self, txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, chara in enumerate(txt):\n",
        "        dig_lst.append(self.char_list.index(chara))\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "  def process_image(self, img):\n",
        "      \"\"\"\n",
        "      Converts image to shape (32, 128, 1) & normalize\n",
        "      \"\"\"\n",
        "      w, h = img.shape\n",
        "      \n",
        "      # image thresholding\n",
        "  #     _, img = cv2.threshold(img, \n",
        "  #                            128, \n",
        "  #                            255, \n",
        "  #                            cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "      \n",
        "      # Aspect Ratio Calculation\n",
        "      new_w = 32\n",
        "      new_h = int(h * (new_w / w))\n",
        "      img = cv2.resize(img, (new_h, new_w))\n",
        "      w, h = img.shape\n",
        "      \n",
        "      img = img.astype('float32')\n",
        "      \n",
        "      # Converts each to (32, 128, 1)\n",
        "      if w < 32:\n",
        "          add_zeros = np.full((32-w, h), 255)\n",
        "          img = np.concatenate((img, add_zeros))\n",
        "          w, h = img.shape\n",
        "      \n",
        "      if h < 128:\n",
        "          add_zeros = np.full((w, 128-h), 255)\n",
        "          img = np.concatenate((img, add_zeros), axis=1)\n",
        "          w, h = img.shape\n",
        "          \n",
        "      if h > 128 or w > 32:\n",
        "          dim = (128,32)\n",
        "          img = cv2.resize(img, dim)\n",
        "      \n",
        "      img = cv2.subtract(255, img)\n",
        "      \n",
        "      img = np.expand_dims(img, axis=2)\n",
        "      \n",
        "      # Normalize \n",
        "      img = img / 255\n",
        "      \n",
        "      return img\n",
        "\n",
        "  def train_valid_split(self):\n",
        "      train_images = []\n",
        "      train_labels = []\n",
        "      train_input_length = []\n",
        "      train_label_length = []\n",
        "      train_original_text = []\n",
        "\n",
        "      valid_images = []\n",
        "      valid_labels = []\n",
        "      valid_input_length = []\n",
        "      valid_label_length = []\n",
        "      valid_original_text = []\n",
        "\n",
        "      words = self.extract_words_from_directories()\n",
        "      for index, word in enumerate(words):\n",
        "        image_filepath = word['image_filepath']\n",
        "        text = word['text']\n",
        "\n",
        "        # processing on image\n",
        "        img = cv2.imread(image_filepath, cv2.IMREAD_GRAYSCALE)\n",
        "        try:\n",
        "            img = self.process_image(img)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # processing on label\n",
        "        try:\n",
        "            label = self.encode_to_labels(text)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        # added this line to prevent error from CTC computation\n",
        "        if len(label) > 31:\n",
        "          continue\n",
        "        \n",
        "        if index % 10 == 0:\n",
        "            valid_images.append(img)\n",
        "            valid_labels.append(label)\n",
        "            valid_input_length.append(31)\n",
        "            valid_label_length.append(len(text))\n",
        "            valid_original_text.append(text)\n",
        "        else:\n",
        "            train_images.append(img)\n",
        "            train_labels.append(label)\n",
        "            train_input_length.append(31)\n",
        "            train_label_length.append(len(text))\n",
        "            train_original_text.append(text)\n",
        "        \n",
        "        if len(text) > self.max_label_len:\n",
        "            self.max_label_len = len(text)\n",
        "    \n",
        "      train_padded_labels = pad_sequences(train_labels, \n",
        "                            maxlen=self.max_label_len, \n",
        "                            padding='post',\n",
        "                            value=len(self.char_list))\n",
        "\n",
        "      valid_padded_labels = pad_sequences(valid_labels, \n",
        "                            maxlen=self.max_label_len, \n",
        "                            padding='post',\n",
        "                            value=len(self.char_list))\n",
        "\n",
        "      # Convert to numpy array.\n",
        "      train_images = np.asarray(train_images)\n",
        "      train_input_length = np.asarray(train_input_length)\n",
        "      train_label_length = np.asarray(train_label_length)\n",
        "\n",
        "      valid_images = np.asarray(valid_images)\n",
        "      valid_input_length = np.asarray(valid_input_length)\n",
        "      valid_label_length = np.asarray(valid_label_length)\n",
        "\n",
        "      train_data = {\n",
        "          \"train_images\": train_images,\n",
        "          \"train_padded_labels\": train_padded_labels,\n",
        "          \"train_input_length\": train_input_length,\n",
        "          \"train_label_length\": train_label_length,\n",
        "          \"train_original_text\": train_original_text\n",
        "      }\n",
        "\n",
        "      valid_data = {\n",
        "          \"valid_images\": valid_images,\n",
        "          \"valid_padded_labels\": valid_padded_labels,\n",
        "          \"valid_input_length\": valid_input_length,\n",
        "          \"valid_label_length\": valid_label_length,\n",
        "          \"valid_original_text\": valid_original_text\n",
        "      }\n",
        "\n",
        "      return train_data, valid_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNN:\n",
        "  def __init__(self, max_label_len, char_list):\n",
        "    self.max_label_len = max_label_len\n",
        "    # input with shape of height=32 and width=128, #RGB would have a depth of 3, Grey scale would only have a depth of 1\n",
        "    self.inputs = Input(shape=(32,128,1))\n",
        "    \n",
        "    # convolution layer with kernel size (3,3)\n",
        "    self.conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(self.inputs)\n",
        "    # poolig layer with kernel size (2,2)\n",
        "    self.pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(self.conv_1)\n",
        "    \n",
        "    self.conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(self.pool_1)\n",
        "    self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(self.conv_2)\n",
        "    \n",
        "    self.conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(self.pool_2)\n",
        "    \n",
        "    self.conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(self.conv_3)\n",
        "\n",
        "    # poolig layer with kernel size (2,1)\n",
        "    self.pool_4 = MaxPool2D(pool_size=(2, 1))(self.conv_4)\n",
        "    \n",
        "    self.conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(self.pool_4)\n",
        "\n",
        "    # Batch normalization layer\n",
        "    self.batch_norm_5 = BatchNormalization()(self.conv_5)\n",
        "    \n",
        "    self.conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(self.batch_norm_5)\n",
        "    self.batch_norm_6 = BatchNormalization()(self.conv_6)\n",
        "    self.pool_6 = MaxPool2D(pool_size=(2, 1))(self.batch_norm_6)\n",
        "    \n",
        "    self.conv_7 = Conv2D(512, (2,2), activation = 'relu')(self.pool_6)\n",
        "    \n",
        "    self.squeezed = Lambda(lambda x: K.squeeze(x, 1))(self.conv_7)\n",
        "    \n",
        "    # bidirectional LSTM layers with units=128\n",
        "    self.blstm_1 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(self.squeezed)\n",
        "    self.blstm_2 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(self.blstm_1)\n",
        "    \n",
        "    self.outputs = Dense(len(char_list)+1, activation = 'softmax')(self.blstm_2)\n",
        "  \n",
        "  def build_valid_model(self, weight_path):\n",
        "    # model to be used at test time\n",
        "    # share the same architecture as train_model\n",
        "    act_model = Model(self.inputs, self.outputs)\n",
        "    act_model.load_weights(weight_path)\n",
        "\n",
        "    return act_model\n",
        "\n",
        "  def ctc_lambda_func(self, args):\n",
        "        y_pred, labels, input_length, label_length = args\n",
        "\n",
        "        return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "  def build_train_model(self):\n",
        "    # loss function\n",
        "    the_labels = Input(name='the_labels', shape=[self.max_label_len], dtype='float32')\n",
        "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "    loss_out = Lambda(self.ctc_lambda_func, output_shape=(1,), name='ctc')([self.outputs, the_labels, input_length, label_length])\n",
        "\n",
        "    #model to be used at training time\n",
        "    model = Model(inputs=[self.inputs, the_labels, input_length, label_length], outputs=loss_out)\n",
        "\n",
        "    return model    "
      ],
      "metadata": {
        "id": "xQlIskX0T6ju"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline:\n",
        "  def __init__(self, dataset_obj, train_data, valid_data, model, sample_size, batch_size, epochs, optimizer_name):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        dataset: Dataset objects which contains the training and validation data\n",
        "        sample_size: sample size to be used for the experiment, ignore the rest of the dataset\n",
        "    \"\"\"\n",
        "    self.model = model\n",
        "    self.dataset_obj = dataset_obj\n",
        "    self.sample_size = sample_size\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "    self.optimizer_name = optimizer_name\n",
        "    self.history = None\n",
        "\n",
        "    # training and validation set to be used for this pipeline\n",
        "    self.train_images = train_data[\"train_images\"][:self.sample_size]\n",
        "    self.train_padded_labels = train_data[\"train_padded_labels\"][:self.sample_size]\n",
        "    self.train_input_length = train_data[\"train_input_length\"][:self.sample_size]\n",
        "    self.train_label_length = train_data[\"train_label_length\"][:self.sample_size]\n",
        "    self.train_original_text = train_data[\"train_original_text\"][:self.sample_size]\n",
        "\n",
        "    self.valid_images = valid_data[\"valid_images\"][:self.sample_size]\n",
        "    self.valid_padded_labels = valid_data[\"valid_padded_labels\"][:self.sample_size]\n",
        "    self.valid_input_length = valid_data[\"valid_input_length\"][:self.sample_size]\n",
        "    self.valid_label_length = valid_data[\"valid_label_length\"][:self.sample_size]\n",
        "    self.valid_original_text = valid_data[\"valid_original_text\"][:self.sample_size]\n",
        "\n",
        "  def train(self, early_stopping=False):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "      best_weight_path (string): path to best training weights\n",
        "      history (obj): history object of the model training\n",
        "    \"\"\"\n",
        "    # build new training model\n",
        "    ocr_model = self.model.build_train_model()\n",
        "\n",
        "    # model compilation\n",
        "    ocr_model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = self.optimizer_name, metrics=['accuracy'])\n",
        "    now = datetime.now()\n",
        "    date_time = now.strftime(\"%m_%d_%Y_%H%M%S\")\n",
        "\n",
        "    filepath=f\"HTR_Using_CRNN/Model/{date_time}.hdf5\"\n",
        "\n",
        "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "    callbacks_list = [checkpoint]\n",
        "    if early_stopping:\n",
        "      earlystopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "      callbacks_list.append(earlystopping)\n",
        "\n",
        "    # training starts here\n",
        "    history = ocr_model.fit(x=[self.train_images, self.train_padded_labels, self.train_input_length, self.train_label_length],\n",
        "                    y=np.zeros(len(self.train_images)),\n",
        "                    batch_size=self.batch_size, \n",
        "                    epochs=self.epochs, \n",
        "                    validation_data=([self.valid_images, self.valid_padded_labels, self.valid_input_length, self.valid_label_length], [np.zeros(len(self.valid_images))]),\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks_list)\n",
        "\n",
        "    return filepath, history\n",
        "\n",
        "  def visualize_prediction(self, weight_path, start_idx, end_idx):\n",
        "    \"\"\"\n",
        "    Plot prediction of images from start_idx: end_idx\n",
        "    \"\"\"\n",
        "    # load the saved best model weights\n",
        "    valid_model = self.model.build_valid_model(weight_path)\n",
        "\n",
        "    # predict outputs on validation images\n",
        "    prediction = valid_model.predict(self.valid_images[start_idx: end_idx])\n",
        "    \n",
        "    # use CTC decoder\n",
        "    decoded = K.ctc_decode(prediction,   \n",
        "                          input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
        "                          greedy=True)[0][0]\n",
        "\n",
        "    out = K.get_value(decoded)\n",
        "\n",
        "    # see the results\n",
        "    for i, x in enumerate(out):\n",
        "        print(\"original_text = \", self.valid_original_text[start_idx+i])\n",
        "        print(\"predicted text = \", end = '')\n",
        "        for p in x:\n",
        "            if int(p) != -1:\n",
        "                print(self.dataset_obj.char_list[int(p)], end = '')\n",
        "        plt.imshow(self.valid_images[start_idx+i].reshape(32,128), cmap=plt.cm.gray)\n",
        "        plt.show()\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "E1Hv9wnPOp20"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils function block\n",
        "\n",
        "def plot_graph(epochs, acc, val_acc, title):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(epochs, acc, 'b')\n",
        "    plt.plot(epochs, val_acc, 'r')\n",
        "    plt.title(title)\n",
        "    plt.ylabel(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dj6omm0rcHw-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = HandwritingDataset(\"/content/xml\", \"/content/words\")\n",
        "train_data, valid_data = dataset.train_valid_split()"
      ],
      "metadata": {
        "id": "cvR9WfD8UX2O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sizes = [10000, 50000, 100000]\n",
        "batch_size = 8\n",
        "epochs = 30\n",
        "optimizer_name = 'sgd'\n",
        "\n",
        "histories = {}\n",
        "pipelines = {}\n",
        "best_weights = {}\n",
        "for sample_size in sample_sizes:\n",
        "  model = CRNN(dataset.max_label_len, dataset.char_list)\n",
        "  pipeline = Pipeline(dataset, train_data, valid_data, model, sample_size, batch_size, epochs, optimizer_name)\n",
        "  best_weight, history = pipeline.train(early_stopping=True)\n",
        "\n",
        "  # store training history for this sample size\n",
        "  histories[sample_size] = history\n",
        "  pipelines[sample_size] = pipeline\n",
        "  best_weights[sample_size] = best_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVF-aROjWYBM",
        "outputId": "6079dcda-39e6-4293-b950-b19a5badf506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 11.99903, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 63s - loss: 13.5910 - accuracy: 0.0013 - val_loss: 11.9990 - val_accuracy: 0.0477 - 63s/epoch - 50ms/step\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 11.99903 to 10.86148, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 43s - loss: 11.1960 - accuracy: 0.0393 - val_loss: 10.8615 - val_accuracy: 0.0399 - 43s/epoch - 35ms/step\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 10.86148 to 9.19942, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 43s - loss: 9.4748 - accuracy: 0.0721 - val_loss: 9.1994 - val_accuracy: 0.1067 - 43s/epoch - 35ms/step\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 9.19942 to 8.40386, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 52s - loss: 7.6667 - accuracy: 0.1201 - val_loss: 8.4039 - val_accuracy: 0.0869 - 52s/epoch - 41ms/step\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 8.40386 to 5.57306, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 43s - loss: 5.9206 - accuracy: 0.1709 - val_loss: 5.5731 - val_accuracy: 0.2200 - 43s/epoch - 34ms/step\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 5.57306 to 5.40854, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 43s - loss: 4.6503 - accuracy: 0.2348 - val_loss: 5.4085 - val_accuracy: 0.2734 - 43s/epoch - 35ms/step\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 5.40854 to 4.33906, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 45s - loss: 3.7902 - accuracy: 0.2880 - val_loss: 4.3391 - val_accuracy: 0.2961 - 45s/epoch - 36ms/step\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 4.33906 to 4.20105, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 43s - loss: 3.1306 - accuracy: 0.3348 - val_loss: 4.2011 - val_accuracy: 0.3351 - 43s/epoch - 35ms/step\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 4.20105 to 4.09078, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 43s - loss: 2.6299 - accuracy: 0.3864 - val_loss: 4.0908 - val_accuracy: 0.3628 - 43s/epoch - 34ms/step\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 4.09078 to 3.82060, saving model to HTR_Using_CRNN/Model/10_23_2022_212708.hdf5\n",
            "1250/1250 - 48s - loss: 2.1540 - accuracy: 0.4423 - val_loss: 3.8206 - val_accuracy: 0.3897 - 48s/epoch - 39ms/step\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss did not improve from 3.82060\n",
            "1250/1250 - 44s - loss: 1.7488 - accuracy: 0.4976 - val_loss: 4.0031 - val_accuracy: 0.3843 - 44s/epoch - 35ms/step\n",
            "Epoch 12/30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline.visualize_prediction(best_weight, 102, 105)"
      ],
      "metadata": {
        "id": "AB4zAygNXypk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1,len(loss)+1)\n",
        "\n",
        "# utils function to visualize training and validation accuracy\n",
        "plot_graph(epochs, acc, val_acc, \"accuracy\")"
      ],
      "metadata": {
        "id": "nOJqXsF1jGZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sXgqKAi-jib1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyM4S0GrMxkfsZQB4rp7kJkA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}